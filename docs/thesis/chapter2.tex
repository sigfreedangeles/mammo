\section{Review of Related Literature}

\qquad In recent years, there have been an influx of deep learning applications from various researchers for medical imaging tasks. To name a few, Penas et. al \cite{malariablood} implemented a CNN to recognize Malaria parasites on microscopic images of thin blood smears; Shichijo et. al \cite{helicobacter} explored the role of artificial intelligence in the diagnosis of Helicobacter pylori gastritis based on endoscopic images; and Chen et. al \cite{thorax} constructed a deep CNN based method for thorax disease diagnosis through chest x-ray images. \\

	Quinn et. al \cite{quinn} also evaluated the performance of deep CNN on three different microscopic tasks: diagnosis of malaria in thick blood smears, tuberculosis in sputum samples, and intestinal parasite egg in stool samples. The authors also explored methods to maximize the performance of the deep CNN they've constructed. In their methodology, the images were downsampled and then split up into overlapping patches, with the downsampling factor and patch size determined by the type of pathogen to be recognised in each case. This is because each image for each case have several objects (plasmodium for Malaria, Tuberculosis bacilli, and eggs of hookworm) of interest. Through this, the authors have obtained a significantly larger data set. Although the potential number of negative patches (i.e. with absence of any of these pathogens, though possible with other types of objects such as staining artifacts, blood cell or impurities) is disproportionately large compared to the number of positive patches since most of each image do not contain pathogen objects, two measures were done by the authors to make the training and testing sets more balanced. First, negative patches were randomly discarded so that there was at most 100 times the number of positive patches. Second, new positive patches were created by applying data augmenting techniques such as rotating, flipping and a combination of both, yielding 7 extra positive patches for each original one. Their method of generating more instances for the data set is especially useful for when the number of images procured is small. For the results of all cases, accuracy is very high with the AOC scores being greater than 0.99. \\

	Cancer imaging is no exception to deep learning applications. In fact, there have been several studies about deep learning applications for lung cancer, skin cancer, colorectal cancer, bladder cancer, prostate cancer, and breast cancer for the past couple of years. Pearce \cite{pearce} investigated the application of deep learning networks to tumour classification specifically to correctly classify the location of mitoses in slide images. The author found that simple networks can deliver reasonable performance comparable with mid-range performers on the same data set. The simple network constructed in the study is a three-layer convolutional neural network with a binary classifier in the final layer trained on biopsy slide images. The author also explored ways in which model saturation can be resolved. The study suggests that model saturation can be resolved by a combination of limiting the number of parameters in the model, ensuring that training data is balanced between positive and negative instances, low learning rates, and iteratively biasing the input data towards examples that thte model has incorrectly classified after previous training epochs (supervised learning). The resulting model performed reasonably well scoring a 72\% accuracy. \\

	Computer imaging methods in the past fail to consider the difference between natural images and medical images for training CNNs. Moreover, when examining medical images, a set of views must be fused in order to perform a proper reading. Geras et. al \cite{geras} proposed to use a multi-view deep CNN that handles a set of high-resolution medical images specifically a large-scale mammography-based breast cancer screening data set consisting of 886,000 images. The authors built a CNN that classifies an input image as BI-RADS 0 ("incomplete"), BI-RADS 1 ("normal"), or BI-RADS 2 ("benign finding"). The novelty of the authors' method here is that instead of heavily downscaling an original high-resolution image, they kept the input at 2600 x 2000 pixels. Heavily downscaling an image is a common method in object recognition and detection in order to improve the computational efficiency, both in terms of computation and memory, and also because no further improvement has been attributed with higher-resolution images. However, the authors argued that this kind of method works well with natural images where the objects of interest are usually presented in larger portions than other objects, and what matters most are their macro-structures such as shapes, colors, etc. On the other hand, medical images, where there are fine details to consider, do no share these properties with natural images, so the downscaling of medical images are not desirable. The authors used aggresive convolution layers and pooling layers specifically convolution layers with strides larger than one in the first two convolutional layers and the first pooling layer with a larger stride than the other pooling layers. An experiment was conducted where the accuracies where evaluated with different decreases in resolution of the image. The results are that when the images where downscaled by x 1/8, the peak accuracy was 74.3\%. On the other hand, when the images are not downscaled, the peak accuracy was 78.7\%. \\

	In training and fine tuning CNNs, a large amount of labeled data is always required. However, for most medical image projects, acquiring such a database is very difficult. Sun et. al \cite{sun} developed a graph-based semi-supervised learning (SSL) scheme using deep CNN for breast cancer diagnosis. SSL is a technique in machine learning wherein labeled data and unlabeled data are employed in the training of the model. Here, the authors' proposed scheme only requires a small portion of labeled data (100 instances). This is potentially useful and powerful since the process of diagnosing and labeling hundreds or even thousands of data is costly. Consequently, their CNN model achieved an accuracy of 0.8243 and an area under the ROC curve of 0.8818. \\

	According to Chougrad et. al \cite{chougrad}, radiologists often find it difficult to classify mammography mass lesions. So, the authors constructed a Computer Assisted Diagnosis (CAD) system based on a deep CNN model that classifies breast mass lesions. The authors proposed the use of transfer learning with exponentially decaying learning rate per layer. The use of transfer learning was justified by the small size of the data set only comprising of 600 images, 300 images showing benign lesions and 300 showing malignant lesions. Then, when fine-tuning the CNN, the weights of the last convolutional layers need to be tweaked as much as possible, while the first ones remain nearly untouched. They propose to do this by the aforementioned per-layer exponentially decaying learning rate. The rationale for this was that the first layers of CNN learn generic features, while the last layers tend to be more specific to the data hence by fine-tuning the last convolutional layers, the model learns more data-specific features. With the setup they've proposed, their model achieved an accuracy of 98.23\% on an independent, test database. \\

	Araújo et. al \cite{araujo} constructed a CNN for a different task specifically the classification of hematoxylin and eosin stained breast biopsy images into four classes namely normal tissue, benign lesion, in situ carcinoma and invasive carcinoma, and in two classes, carcinoma and non-carcinoma. The architecture of the CNN was custom-tailored to retrieve information at nuclei level and overall tissue organization. This is performed by first processing several patches obtained from an input image with a patch-wise classifier, and then combining the classification results of all the image patches to obtain the final image-wise classification. In addition, the author evaluated the performance of two network architecture, a CNN and a CNN with the feed-forward neural network swapped with a Support Vector Machine (SVM) classifier. Accuracies of 77.8\% for four class and 83.3\% for carcinoma/non-carcinoma are achieved. \\

	Similarly, Rakhlin et. al \cite{rakhlin} had similar objectives to that of Araújo et. al which was to develop a deep CNN for the classification of hematoxylin and eosin stained breast biopsy images into four classes (normal tissue, benign lesion, in situ carcinoma and invasive carcinoma) and in two classes (carcinoma and non-carcinoma). They both had the same data set, 400 images of 4 classes. To remedy the problem of overfitting due to a small data set, the authors employed an approach known as deep convolutional feature representation by which deep CNNs, trained on large and general datasets like ImageNet, are used for unsupervised feature representation extraction. They also used Light GBM, a fast, distributed, high performance implementation of gradient boosted trees, for supervised classification. The results of their model outperformed that of \cite{araujo} having an accuracy of 87.2\% for the 4-class classification task and 93.8\% accuracy for the 2-class classification. \\

	In 2015, He et. al \cite{Resnet} proposed a residual learning framework to achieve deeper CNNs without dealing with vanishing and exploding gradients when training very deep CNNs. They constructed a CNN with shortcut connections that skip one or more stacked layers through use of identity mapping where the outputs of shortcut connections are added to the outputs of the stacked layers. The resulting network is referred to as a residual neural network (RNN). The author's constructed model, a 152-layer RNN, surpassed human-level performance on the ImageNet classification data set with an accuracy of 0.89. \\

	Unfortunately, these studies do not discuss the implementation of their model into a widely-distributed system. However, Weng \cite{weng} was tasked to develop a CNN by iSono Health, a startup company, who developed the iSono app, an affordable, automated ultrasound imaging platform to assist women with monthly self-monitoring of breast cancer detection. The application comes with a device that allows user to self-scan through ultrasound. The resulting images are sent to the user's physician who sends back a diagnosis regarding the images. A well-developed CNN could potentially remove the need for a physician, for the physician is not always readily-available. The objective of the CNN was to differentiate benign and malignant breast lesions. Furthermore, the author compared the performance of a CNN with respect to a fully-connected neural network (FCNN). The CNN model constructed achieved an accuracy of 73\% while the FCNN only achieved 66\%. In the author's paper, there was no discussion regarding the specifics of incorporating the constructed model into the iSono app.  Moreover, the CNN model achieved a relatively low accuracy compared to other models mentioned as the model was built from scratch, and the design was inspired by AlexNet. \\

	Large-scale medical image databases with fully-annotated ROIs are scarce. There are only a handful of such databases that are publicly available such as CBIS-DDSM. It would also be unrealistic to require all existing databases to be fully annotated with ROIs. In \cite{CNNmodel}, Shen developed an end-to-end training algorithm for whole-image breast cancer diagnosis based on mammograms. The authors achieved a trainable whole image classifier model from patch-wise classifier model greatly reducing reliance on data sets with lesion annotations as there are only a few databases that exist with such ROI annotations. This is done by training a patch-wise classifier model first then adding a new convolutional layer on top of the patch-wise classifier. The best constructed whole image classifier model achieved an AUC score of 0.88 on DDSM (Digital Database for Screening Mammography). And, the model was trained as well on INbreast, a database with whole image annotations, which achieved an AUC score of 0.96.